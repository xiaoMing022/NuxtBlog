---
title: Prometheus
date:  2025/9/18
description: äº†è§£Prometheusçš„ä½¿ç”¨
image: /blogs-img/promethues.png
alt: How to fix vuex type issue
ogImage: /blogs-img/promethues.png
tags: ['promethues']
published: true
---

# Prometheus

## ä¸€ã€å…¥é—¨

![Prometheus architecture](https://prometheus.ac.cn/assets/docs/architecture.svg)

### 1. **Prometheus Serverï¼ˆæ ¸å¿ƒ)**

Prometheus Server æ˜¯ Prometheus ä½“ç³»çš„æ ¸å¿ƒï¼Œå®šæœŸä» Exporters å’Œ Pushgateway æ‹‰å–æ•°æ®ï¼Œå­˜å‚¨åœ¨æ—¶åºæ•°æ®åº“ä¸­ï¼Œå¹¶æ‰§è¡Œè§„åˆ™è®¡ç®—ï¼Œå†…éƒ¨åŒ…å«ä¸¤ä¸ªä¸»è¦æ¨¡å—ï¼š

**Retrievalï¼ˆæŠ“å–æ¨¡å—ï¼‰**

- å®šæœŸå‘å„ä¸ªç›‘æ§ç›®æ ‡ï¼ˆTargetï¼‰å‘é€ HTTP è¯·æ±‚ï¼ŒæŠ“å–ç›‘æ§æ•°æ®
- æŠ“å–çš„åœ°å€å’Œé—´éš”åœ¨ `prometheus.yml` ä¸­ `scrape_configs` é‡Œå®šä¹‰

**TSDBï¼ˆæ—¶é—´åºåˆ—æ•°æ®åº“ï¼‰**

- æŠŠæŠ“å–åˆ°çš„ç›‘æ§æ•°æ®æŒ‰æ—¶é—´åºåˆ—å­˜å‚¨
- è‡ªåŠ¨è¿›è¡Œå‹ç¼©å’Œåˆ†ç‰‡ï¼Œæ”¯æŒé«˜æ•ˆçš„æ—¶é—´åŒºé—´æŸ¥è¯¢
- é»˜è®¤æœ¬åœ°å­˜å‚¨ï¼Œæ”¯æŒæŒä¹…åŒ–å’Œè¿œç¨‹å­˜å‚¨æ’ä»¶

### 2. PushGatewayå’Œ**Exportersï¼ˆæ•°æ®é‡‡é›†å™¨ï¼‰**

Exporter æ˜¯éƒ¨ç½²åœ¨è¢«ç›‘æ§ç›®æ ‡ä¸Šçš„å°æœåŠ¡ï¼Œç”¨äº**æŠŠç³»ç»ŸæŒ‡æ ‡æš´éœ²ä¸º Prometheus èƒ½ç†è§£çš„æ ¼å¼ï¼ˆ/metricsï¼‰**ã€‚

- å¸¸è§ Exporterï¼š
  - Node Exporterï¼šç›‘æ§ä¸»æœºçš„ CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œç­‰
  - Blackbox Exporterï¼šç›‘æ§ HTTPã€DNSã€TCP å¯ç”¨æ€§
  - MySQL Exporter / Redis Exporterï¼šç›‘æ§ç‰¹å®šæœåŠ¡çš„çŠ¶æ€

PushGateWayä¸»è¦é€‚ç”¨äºçŸ­æ—¶ä»»åŠ¡ï¼Œè€ŒExporterä¸»è¦é€‚ç”¨äºé•¿æ—¶é—´æŠ“å–ä»»åŠ¡

### 3. Service Discoveryï¼ˆæœåŠ¡å‘ç°ï¼‰

è‡ªåŠ¨å‘ç°ç›®æ ‡èŠ‚ç‚¹ï¼ˆtargetsï¼‰çš„æœºåˆ¶,æ”¯æŒé™æ€é…ç½®ã€DNSã€Kubernetesã€Consul ç­‰æ–¹å¼,ç”¨äºé¿å…æ‰‹åŠ¨ç»´æŠ¤å¤§é‡ target åˆ—è¡¨

### 4. PromQLï¼ˆæŸ¥è¯¢è¯­è¨€ï¼‰

Prometheus å†…ç½®çš„æŸ¥è¯¢è¯­è¨€

- ç”¨äºï¼š

  - åœ¨ç½‘é¡µ UI æˆ– Grafana ä¸­å±•ç¤ºæ•°æ®
  - ä½œä¸ºå‘Šè­¦è§„åˆ™ï¼ˆalert rulesï¼‰æ¡ä»¶è¡¨è¾¾å¼

- ä¾‹å¦‚ï¼š

  ```
  node_cpu_seconds_total{mode="idle"}
  ```

------

### 5. Alertmanagerï¼ˆå‘Šè­¦æ¨¡å—ï¼‰

- æ¥æ”¶ Prometheus å‘é€çš„å‘Šè­¦ï¼ˆalertsï¼‰
- è´Ÿè´£ï¼š
  - å‘Šè­¦å»é‡ã€èšåˆã€æŠ‘åˆ¶
  - å‘é€é€šçŸ¥ï¼ˆé‚®ä»¶ / Webhook / Slack / é£ä¹¦ / Telegram ç­‰ï¼‰
- é€šè¿‡ `alerting` é…ç½®ä¸ Prometheus server è¿æ¥

------

### 6. Grafanaï¼ˆå¤–éƒ¨å¯è§†åŒ–ï¼‰

- è™½ç„¶ä¸å±äº Prometheus æ ¸å¿ƒæ¨¡å—ï¼Œä½†æœ€å¸¸ä¸ Prometheus æ­é…ä½¿ç”¨
- ç”¨äºï¼š
  - ç¼–å†™ Dashboardï¼ˆä»ªè¡¨ç›˜ï¼‰å±•ç¤ºæ•°æ®
  - æ¥æ”¶ Prometheus æ•°æ®æº

------

### ğŸ“ æ€»ç»“

| ç»„ä»¶                  | ä½œç”¨                    |
| --------------------- | ----------------------- |
| **Prometheus Server** | æŠ“å–æŒ‡æ ‡ + å­˜å‚¨æ•°æ®     |
| **Exporters**         | æŠŠä¸»æœº/æœåŠ¡çŠ¶æ€æš´éœ²å‡ºæ¥ |
| **Service Discovery** | è‡ªåŠ¨å‘ç°ç›‘æ§ç›®æ ‡        |
| **PromQL**            | æ•°æ®æŸ¥è¯¢ä¸åˆ†æè¯­è¨€      |
| **Alertmanager**      | æ¥æ”¶å‘Šè­¦å¹¶å‘é€é€šçŸ¥      |
| **Grafana**           | ç¾è§‚çš„æ•°æ®å±•ç¤º          |


### ä¸€ä¸ªå°å®ä¾‹
***1. prometheus.ymlé…ç½®æ–‡ä»¶çš„å°å®ä¾‹***

```yaml
# my global config

#global å—æ§åˆ¶ Prometheus æœåŠ¡å™¨çš„å…¨å±€é…ç½®ã€‚
#æˆ‘ä»¬æœ‰ä¸¤ä¸ªé€‰é¡¹ã€‚ç¬¬ä¸€ä¸ªæ˜¯ scrape_intervalï¼Œå®ƒæ§åˆ¶ Prometheus æŠ“å–ç›®æ ‡çš„é¢‘ç‡ã€‚
#æ‚¨å¯ä»¥ä¸ºå•ä¸ªç›®æ ‡è¦†ç›–æ­¤è®¾ç½®ã€‚
#åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œå…¨å±€è®¾ç½®ä¸ºæ¯ 15 ç§’æŠ“å–ä¸€æ¬¡ã€‚evaluation_interval é€‰é¡¹æ§åˆ¶ Prometheus è¯„ä¼°è§„åˆ™çš„é¢‘ç‡ã€‚Prometheus ä½¿ç”¨è§„åˆ™åˆ›å»ºæ–°çš„æ—¶é—´åºåˆ—å¹¶ç”Ÿæˆå‘Šè­¦ã€‚
global:
  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.
  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.
  # scrape_timeout is set to the global default (10s).

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          # - alertmanager:9093

#rule_files å—æŒ‡å®šäº†æˆ‘ä»¬å¸Œæœ› Prometheus æœåŠ¡å™¨åŠ è½½çš„ä»»ä½•è§„åˆ™çš„ä½ç½®ã€‚
#ç›®å‰æˆ‘ä»¬è¿˜æ²¡æœ‰è§„åˆ™ã€‚
rule_files:
  # - "first_rules.yml"
  # - "second_rules.yml"



#scrape_configsï¼Œæ§åˆ¶ Prometheus ç›‘æ§å“ªäº›èµ„æºã€‚
#ç”±äº Prometheus è‡ªèº«ä¹Ÿé€šè¿‡ HTTP ç«¯ç‚¹æš´éœ²æ•°æ®ï¼Œå› æ­¤å®ƒå¯ä»¥æŠ“å–å¹¶ç›‘æ§è‡ªèº«çš„å¥åº·çŠ¶å†µã€‚
#åœ¨é»˜è®¤é…ç½®ä¸­ï¼Œæœ‰ä¸€ä¸ªåä¸º prometheus çš„å•ä¸ªä½œä¸šï¼Œå®ƒæŠ“å– Prometheus æœåŠ¡å™¨æš´éœ²çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚
#è¯¥ä½œä¸šåŒ…å«ä¸€ä¸ªé™æ€é…ç½®çš„ç›®æ ‡ï¼Œå³ç«¯å£ 9090 ä¸Šçš„ localhostã€‚
scrape_configs:

  # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
  - job_name: "prometheus"

  # metrics_path defaults to '/metrics'
  # scheme defaults to 'http'.

    static_configs:
      - targets: ["localhost:9090"]
       # The label name is added as a label `label_name=<label_value>` to any timeseries scraped from this config.
        labels:
          app: "prometheus"

```

***2. é…ç½® Prometheus ç›‘æ§ç¤ºä¾‹ç›®æ ‡***

ç°åœ¨æˆ‘ä»¬å°†é…ç½® Prometheus æ¥æŠ“å–è¿™äº›æ–°ç›®æ ‡ã€‚è®©æˆ‘ä»¬å°†æ‰€æœ‰ä¸‰ä¸ªç«¯ç‚¹åˆ†ç»„åˆ°ä¸€ä¸ªåä¸º `node` çš„ä½œä¸šä¸­ã€‚æˆ‘ä»¬å‡è®¾å‰ä¸¤ä¸ªç«¯ç‚¹æ˜¯ç”Ÿäº§ç›®æ ‡ï¼Œè€Œç¬¬ä¸‰ä¸ªåˆ™ä»£è¡¨ä¸€ä¸ªé‡‘ä¸é›€å®ä¾‹ã€‚ä¸ºäº†åœ¨ Prometheus ä¸­å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥å‘å•ä¸ªä½œä¸šæ·»åŠ å¤šä¸ªç«¯ç‚¹ç»„ï¼Œå¹¶ä¸ºæ¯ä¸ªç›®æ ‡ç»„æ·»åŠ é¢å¤–çš„æ ‡ç­¾ã€‚åœ¨æ­¤ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°† `group="production"` æ ‡ç­¾æ·»åŠ åˆ°ç¬¬ä¸€ç»„ç›®æ ‡ï¼ŒåŒæ—¶å°† `group="canary"` æ·»åŠ åˆ°ç¬¬äºŒç»„ã€‚

ä¸ºæ­¤ï¼Œè¯·å°†ä»¥ä¸‹ä½œä¸šå®šä¹‰æ·»åŠ åˆ°æ‚¨çš„ `prometheus.yml` æ–‡ä»¶ä¸­çš„ `scrape_configs` éƒ¨åˆ†ï¼Œç„¶åé‡æ–°å¯åŠ¨æ‚¨çš„ Prometheus å®ä¾‹

```yaml
scrape_configs:
  - job_name:       'node'

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
      - targets: ['localhost:8080', 'localhost:8081']
        labels:
          group: 'production'

      - targets: ['localhost:8082']
        labels:
          group: 'canary'

```

***3.å¯¹Prometheusé…ç½®è®°å½•è§„åˆ™***

åˆ›å»ºä¸€ä¸ªåŒ…å«ä»¥ä¸‹è®°å½•è§„åˆ™çš„æ–‡ä»¶ï¼Œå¹¶å°†å…¶ä¿å­˜ä¸º `prometheus.rules.yml`

```yaml
groups:
- name: cpu-node
  rules:
  - record: job_instance_mode:node_cpu_seconds:avg_rate5m
    expr: avg by (job, instance, mode) (rate(node_cpu_seconds_total[5m]))
```

åŒæ—¶åœ¨`prometheus.yml` ä¸­æ·»åŠ  `rule_files` è¯­å¥ã€‚åŒæ—¶éœ€é‡å¯Prometheusæ¥åŠ è½½é…ç½®é¡¹

```yaml
global:
  scrape_interval:     15s # By default, scrape targets every 15 seconds.
  evaluation_interval: 15s # Evaluate rules every 15 seconds.

  # Attach these extra labels to all timeseries collected by this Prometheus instance.
  external_labels:
    monitor: 'codelab-monitor'

rule_files:
  - 'prometheus.rules.yml'

scrape_configs:
  - job_name: 'prometheus'

    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
      - targets: ['localhost:9090']



  - job_name:       'node'
    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s

    static_configs:
      - targets: ['localhost:8080', 'localhost:8081']
        labels:
          group: 'production'

      - targets: ['localhost:8082']
        labels:
          group: 'canary'
```

## äºŒã€é…ç½®

Prometheus é€šè¿‡å‘½ä»¤è¡Œæ ‡å¿—å’Œé…ç½®æ–‡ä»¶è¿›è¡Œé…ç½®ã€‚å¦‚æœæ–°é…ç½®æ ¼å¼ä¸æ­£ç¡®ï¼Œåˆ™æ›´æ”¹å°†ä¸ä¼šåº”ç”¨ã€‚é€šè¿‡å‘ Prometheus è¿›ç¨‹å‘é€ `SIGHUP` æˆ–å‘ `/-/reload` ç«¯ç‚¹å‘é€ HTTP POST è¯·æ±‚ï¼ˆå½“å¯ç”¨äº† `--web.enable-lifecycle` æ ‡å¿—æ—¶ï¼‰æ¥è§¦å‘é…ç½®é‡æ–°åŠ è½½ã€‚è¿™ä¹Ÿä¼šé‡æ–°åŠ è½½ä»»ä½•å·²é…ç½®çš„è§„åˆ™æ–‡ä»¶ã€‚

### 1.global(å…¨å±€é…ç½®)

```yaml
global:
  # How frequently to scrape targets by default.
  [ scrape_interval: <duration> | default = 1m ]

  # How long until a scrape request times out.
  # It cannot be greater than the scrape interval.
  [ scrape_timeout: <duration> | default = 10s ]

  # The protocols to negotiate during a scrape with the client.
  # Supported values (case sensitive): PrometheusProto, OpenMetricsText0.0.1,
  # OpenMetricsText1.0.0, PrometheusText0.0.4.
  # The default value changes to [ PrometheusProto, OpenMetricsText1.0.0, OpenMetricsText0.0.1, PrometheusText0.0.4 ]
  # when native_histogram feature flag is set.
  [ scrape_protocols: [<string>, ...] | default = [ OpenMetricsText1.0.0, OpenMetricsText0.0.1, PrometheusText0.0.4 ] ]

  # How frequently to evaluate rules.
  [ evaluation_interval: <duration> | default = 1m ]

  # Offset the rule evaluation timestamp of this particular group by the
  # specified duration into the past to ensure the underlying metrics have
  # been received. Metric availability delays are more likely to occur when
  # Prometheus is running as a remote write target, but can also occur when
  # there's anomalies with scraping.
  [ rule_query_offset: <duration> | default = 0s ]

  # The labels to add to any time series or alerts when communicating with
  # external systems (federation, remote storage, Alertmanager).
  # Environment variable references `${var}` or `$var` are replaced according
  # to the values of the current environment variables.
  # References to undefined variables are replaced by the empty string.
  # The `$` character can be escaped by using `$$`.
  external_labels:
    [ <labelname>: <labelvalue> ... ]

  # File to which PromQL queries are logged.
  # Reloading the configuration will reopen the file.
  [ query_log_file: <string> ]

  # File to which scrape failures are logged.
  # Reloading the configuration will reopen the file.
  [ scrape_failure_log_file: <string> ]

  # An uncompressed response body larger than this many bytes will cause the
  # scrape to fail. 0 means no limit. Example: 100MB.
  # This is an experimental feature, this behaviour could
  # change or be removed in the future.
  [ body_size_limit: <size> | default = 0 ]

  # Per-scrape limit on the number of scraped samples that will be accepted.
  # If more than this number of samples are present after metric relabeling
  # the entire scrape will be treated as failed. 0 means no limit.
  [ sample_limit: <int> | default = 0 ]

  # Limit on the number of labels that will be accepted per sample. If more
  # than this number of labels are present on any sample post metric-relabeling,
  # the entire scrape will be treated as failed. 0 means no limit.
  [ label_limit: <int> | default = 0 ]

  # Limit on the length (in bytes) of each individual label name. If any label
  # name in a scrape is longer than this number post metric-relabeling, the
  # entire scrape will be treated as failed. Note that label names are UTF-8
  # encoded, and characters can take up to 4 bytes. 0 means no limit.
  [ label_name_length_limit: <int> | default = 0 ]

  # Limit on the length (in bytes) of each individual label value. If any label
  # value in a scrape is longer than this number post metric-relabeling, the
  # entire scrape will be treated as failed. Note that label values are UTF-8
  # encoded, and characters can take up to 4 bytes. 0 means no limit.
  [ label_value_length_limit: <int> | default = 0 ]

  # Limit per scrape config on number of unique targets that will be
  # accepted. If more than this number of targets are present after target
  # relabeling, Prometheus will mark the targets as failed without scraping them.
  # 0 means no limit. This is an experimental feature, this behaviour could
  # change in the future.
  [ target_limit: <int> | default = 0 ]

  # Limit per scrape config on the number of targets dropped by relabeling
  # that will be kept in memory. 0 means no limit.
  [ keep_dropped_targets: <int> | default = 0 ]

  # Specifies the validation scheme for metric and label names. Either blank or
  # "utf8" for full UTF-8 support, or "legacy" for letters, numbers, colons,
  # and underscores.
  [ metric_name_validation_scheme: <string> | default "utf8" ]

  # Specifies whether to convert all scraped classic histograms into native
  # histograms with custom buckets.
  [ convert_classic_histograms_to_nhcb: <bool> | default = false]

  # Specifies whether to scrape a classic histogram, even if it is also exposed as a native
  # histogram (has no effect without --enable-feature=native-histograms).
  [ always_scrape_classic_histograms: <boolean> | default = false ]


runtime:
  # Configure the Go garbage collector GOGC parameter
  # See: https://tip.golang.org/doc/gc-guide#GOGC
  # Lowering this number increases CPU usage.
  [ gogc: <int> | default = 75 ]

# Rule files specifies a list of globs. Rules and alerts are read from
# all matching files.
rule_files:
  [ - <filepath_glob> ... ]

# Scrape config files specifies a list of globs. Scrape configs are read from
# all matching files and appended to the list of scrape configs.
scrape_config_files:
  [ - <filepath_glob> ... ]

# A list of scrape configurations.
scrape_configs:
  [ - <scrape_config> ... ]

# Alerting specifies settings related to the Alertmanager.
alerting:
  alert_relabel_configs:
    [ - <relabel_config> ... ]
  alertmanagers:
    [ - <alertmanager_config> ... ]

# Settings related to the remote write feature.
remote_write:
  [ - <remote_write> ... ]

# Settings related to the OTLP receiver feature.
# See https://prometheus.ac.cn/docs/guides/opentelemetry/ for best practices.
otlp:
  # Promote specific list of resource attributes to labels.
  # It cannot be configured simultaneously with 'promote_all_resource_attributes: true'.
  [ promote_resource_attributes: [<string>, ...] | default = [ ] ]
  # Promoting all resource attributes to labels, except for the ones configured with 'ignore_resource_attributes'.
  # Be aware that changes in attributes received by the OTLP endpoint may result in time series churn and lead to high memory usage by the Prometheus server.
  # It cannot be set to 'true' simultaneously with 'promote_resource_attributes'.
  [ promote_all_resource_attributes: <boolean> | default = false ]
  # Which resource attributes to ignore, can only be set when 'promote_all_resource_attributes' is true.
  [ ignore_resource_attributes: [<string>, ...] | default = [] ]
  # Configures translation of OTLP metrics when received through the OTLP metrics
  # endpoint. Available values:
  # - "UnderscoreEscapingWithSuffixes" refers to commonly agreed normalization used
  #   by OpenTelemetry in https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/pkg/translator/prometheus
  # - "NoUTF8EscapingWithSuffixes" is a mode that relies on UTF-8 support in Prometheus.
  #   It preserves all special characters like dots, but still adds required metric name suffixes
  #   for units and _total, as UnderscoreEscapingWithSuffixes does.
  # - (EXPERIMENTAL) "NoTranslation" is a mode that relies on UTF-8 support in Prometheus.
  #   It preserves all special character like dots and won't append special suffixes for metric
  #   unit and type.
  #
  #   WARNING: The "NoTranslation" setting has significant known risks and limitations (see https://prometheus.ac.cn/docs/practices/naming/
  #   for details):
  #       * Impaired UX when using PromQL in plain YAML (e.g. alerts, rules, dashboard, autoscaling configuration).
  #       * Series collisions which in the best case may result in OOO errors, in the worst case a silently malformed
  #         time series. For instance, you may end up in situation of ingesting `foo.bar` series with unit
  #         `seconds` and a separate series `foo.bar` with unit `milliseconds`.
  [ translation_strategy: <string> | default = "UnderscoreEscapingWithSuffixes" ]
  # Enables adding "service.name", "service.namespace" and "service.instance.id"
  # resource attributes to the "target_info" metric, on top of converting
  # them into the "instance" and "job" labels.
  [ keep_identifying_resource_attributes: <boolean> | default = false ]
  # Configures optional translation of OTLP explicit bucket histograms into native histograms with custom buckets.
  [ convert_histograms_to_nhcb: <boolean> | default = false ]

# Settings related to the remote read feature.
remote_read:
  [ - <remote_read> ... ]

# Storage related settings that are runtime reloadable.
storage:
  [ tsdb: <tsdb> ]
  [ exemplars: <exemplars> ]

# Configures exporting traces.
tracing:
  [ <tracing_config> ]
```

### 2.`<scrape_config>`

ä¸€ä¸ª `scrape_config` éƒ¨åˆ†æŒ‡å®šäº†ä¸€ç»„ç›®æ ‡å’Œæè¿°å¦‚ä½•æŠ“å–å®ƒä»¬çš„å‚æ•°ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œ***ä¸€ä¸ªæŠ“å–é…ç½®æŒ‡å®šä¸€ä¸ªä½œä¸š***ã€‚åœ¨é«˜çº§é…ç½®ä¸­ï¼Œè¿™å¯èƒ½ä¼šæœ‰æ‰€ä¸åŒã€‚

```yaml
# The job name assigned to scraped metrics by default.
job_name: <job_name>

# How frequently to scrape targets from this job.
[ scrape_interval: <duration> | default = <global_config.scrape_interval> ]

# Per-scrape timeout when scraping this job.
# It cannot be greater than the scrape interval.
[ scrape_timeout: <duration> | default = <global_config.scrape_timeout> ]

# The protocols to negotiate during a scrape with the client.
# Supported values (case sensitive): PrometheusProto, OpenMetricsText0.0.1,
# OpenMetricsText1.0.0, PrometheusText0.0.4, PrometheusText1.0.0.
[ scrape_protocols: [<string>, ...] | default = <global_config.scrape_protocols> ]

# Fallback protocol to use if a scrape returns blank, unparseable, or otherwise
# invalid Content-Type.
# Supported values (case sensitive): PrometheusProto, OpenMetricsText0.0.1,
# OpenMetricsText1.0.0, PrometheusText0.0.4, PrometheusText1.0.0.
[ fallback_scrape_protocol: <string> ]

# Whether to scrape a classic histogram, even if it is also exposed as a native
# histogram (has no effect without --enable-feature=native-histograms).
[ always_scrape_classic_histograms: <boolean> |
default = <global.always_scrape_classic_hisotgrams> ]

# The HTTP resource path on which to fetch metrics from targets.
[ metrics_path: <path> | default = /metrics ]

# honor_labels controls how Prometheus handles conflicts between labels that are
# already present in scraped data and labels that Prometheus would attach
# server-side ("job" and "instance" labels, manually configured target
# labels, and labels generated by service discovery implementations).
#
# If honor_labels is set to "true", label conflicts are resolved by keeping label
# values from the scraped data and ignoring the conflicting server-side labels.
#
# If honor_labels is set to "false", label conflicts are resolved by renaming
# conflicting labels in the scraped data to "exported_<original-label>" (for
# example "exported_instance", "exported_job") and then attaching server-side
# labels.
#
# Setting honor_labels to "true" is useful for use cases such as federation and
# scraping the Pushgateway, where all labels specified in the target should be
# preserved.
#
# Note that any globally configured "external_labels" are unaffected by this
# setting. In communication with external systems, they are always applied only
# when a time series does not have a given label yet and are ignored otherwise.
[ honor_labels: <boolean> | default = false ]

# honor_timestamps controls whether Prometheus respects the timestamps present
# in scraped data.
#
# If honor_timestamps is set to "true", the timestamps of the metrics exposed
# by the target will be used.
#
# If honor_timestamps is set to "false", the timestamps of the metrics exposed
# by the target will be ignored.
[ honor_timestamps: <boolean> | default = true ]

# track_timestamps_staleness controls whether Prometheus tracks staleness of
# the metrics that have an explicit timestamps present in scraped data.
#
# If track_timestamps_staleness is set to "true", a staleness marker will be
# inserted in the TSDB when a metric is no longer present or the target
# is down.
[ track_timestamps_staleness: <boolean> | default = false ]

# Configures the protocol scheme used for requests.
[ scheme: <scheme> | default = http ]

# Optional HTTP URL parameters.
params:
  [ <string>: [<string>, ...] ]

# If enable_compression is set to "false", Prometheus will request uncompressed
# response from the scraped target.
[ enable_compression: <boolean> | default = true ]

# File to which scrape failures are logged.
# Reloading the configuration will reopen the file.
[ scrape_failure_log_file: <string> ]

# HTTP client settings, including authentication methods (such as basic auth and
# authorization), proxy configurations, TLS options, custom HTTP headers, etc.
[ <http_config> ]

# List of Azure service discovery configurations.
azure_sd_configs:
  [ - <azure_sd_config> ... ]

# List of Consul service discovery configurations.
consul_sd_configs:
  [ - <consul_sd_config> ... ]

# List of DigitalOcean service discovery configurations.
digitalocean_sd_configs:
  [ - <digitalocean_sd_config> ... ]

# List of Docker service discovery configurations.
docker_sd_configs:
  [ - <docker_sd_config> ... ]

# List of Docker Swarm service discovery configurations.
dockerswarm_sd_configs:
  [ - <dockerswarm_sd_config> ... ]

# List of DNS service discovery configurations.
dns_sd_configs:
  [ - <dns_sd_config> ... ]

# List of EC2 service discovery configurations.
ec2_sd_configs:
  [ - <ec2_sd_config> ... ]

# List of Eureka service discovery configurations.
eureka_sd_configs:
  [ - <eureka_sd_config> ... ]

# List of file service discovery configurations.
file_sd_configs:
  [ - <file_sd_config> ... ]

# List of GCE service discovery configurations.
gce_sd_configs:
  [ - <gce_sd_config> ... ]

# List of Hetzner service discovery configurations.
hetzner_sd_configs:
  [ - <hetzner_sd_config> ... ]

# List of HTTP service discovery configurations.
http_sd_configs:
  [ - <http_sd_config> ... ]


# List of IONOS service discovery configurations.
ionos_sd_configs:
  [ - <ionos_sd_config> ... ]

# List of Kubernetes service discovery configurations.
kubernetes_sd_configs:
  [ - <kubernetes_sd_config> ... ]

# List of Kuma service discovery configurations.
kuma_sd_configs:
  [ - <kuma_sd_config> ... ]

# List of Lightsail service discovery configurations.
lightsail_sd_configs:
  [ - <lightsail_sd_config> ... ]

# List of Linode service discovery configurations.
linode_sd_configs:
  [ - <linode_sd_config> ... ]

# List of Marathon service discovery configurations.
marathon_sd_configs:
  [ - <marathon_sd_config> ... ]

# List of AirBnB's Nerve service discovery configurations.
nerve_sd_configs:
  [ - <nerve_sd_config> ... ]

# List of Nomad service discovery configurations.
nomad_sd_configs:
  [ - <nomad_sd_config> ... ]

# List of OpenStack service discovery configurations.
openstack_sd_configs:
  [ - <openstack_sd_config> ... ]

# List of OVHcloud service discovery configurations.
ovhcloud_sd_configs:
  [ - <ovhcloud_sd_config> ... ]

# List of PuppetDB service discovery configurations.
puppetdb_sd_configs:
  [ - <puppetdb_sd_config> ... ]

# List of Scaleway service discovery configurations.
scaleway_sd_configs:
  [ - <scaleway_sd_config> ... ]

# List of Zookeeper Serverset service discovery configurations.
serverset_sd_configs:
  [ - <serverset_sd_config> ... ]

# List of STACKIT service discovery configurations.
stackit_sd_configs:
  [ - <stackit_sd_config> ... ]

# List of Triton service discovery configurations.
triton_sd_configs:
  [ - <triton_sd_config> ... ]

# List of Uyuni service discovery configurations.
uyuni_sd_configs:
  [ - <uyuni_sd_config> ... ]

# List of labeled statically configured targets for this job.
static_configs:
  [ - <static_config> ... ]

# List of target relabel configurations.
relabel_configs:
  [ - <relabel_config> ... ]

# List of metric relabel configurations.
metric_relabel_configs:
  [ - <relabel_config> ... ]

# An uncompressed response body larger than this many bytes will cause the
# scrape to fail. 0 means no limit. Example: 100MB.
# This is an experimental feature, this behaviour could
# change or be removed in the future.
[ body_size_limit: <size> | default = 0 ]

# Per-scrape limit on the number of scraped samples that will be accepted.
# If more than this number of samples are present after metric relabeling
# the entire scrape will be treated as failed. 0 means no limit.
[ sample_limit: <int> | default = 0 ]

# Limit on the number of labels that will be accepted per sample. If more
# than this number of labels are present on any sample post metric-relabeling,
# the entire scrape will be treated as failed. 0 means no limit.
[ label_limit: <int> | default = 0 ]

# Limit on the length (in bytes) of each individual label name. If any label
# name in a scrape is longer than this number post metric-relabeling, the
# entire scrape will be treated as failed. Note that label names are UTF-8
# encoded, and characters can take up to 4 bytes. 0 means no limit.
[ label_name_length_limit: <int> | default = 0 ]

# Limit on the length (in bytes) of each individual label value. If any label
# value in a scrape is longer than this number post metric-relabeling, the
# entire scrape will be treated as failed. Note that label values are UTF-8
# encoded, and characters can take up to 4 bytes. 0 means no limit.
[ label_value_length_limit: <int> | default = 0 ]

# Limit per scrape config on number of unique targets that will be
# accepted. If more than this number of targets are present after target
# relabeling, Prometheus will mark the targets as failed without scraping them.
# 0 means no limit. This is an experimental feature, this behaviour could
# change in the future.
[ target_limit: <int> | default = 0 ]

# Limit per scrape config on the number of targets dropped by relabeling
# that will be kept in memory. 0 means no limit.
[ keep_dropped_targets: <int> | default = 0 ]

# Specifies the validation scheme for metric and label names. Either blank or
# "utf8" for full UTF-8 support, or "legacy" for letters, numbers, colons, and
# underscores.
[ metric_name_validation_scheme: <string> | default "utf8" ]

# Specifies the character escaping scheme that will be requested when scraping
# for metric and label names that do not conform to the legacy Prometheus
# character set. Available options are:
#   * `allow-utf-8`: Full UTF-8 support, no escaping needed.
#   * `underscores`: Escape all legacy-invalid characters to underscores.
#   * `dots`: Escapes dots to `_dot_`, underscores to `__`, and all other
#     legacy-invalid characters to underscores.
#   * `values`: Prepend the name with `U__` and replace all invalid
#     characters with their unicode value, surrounded by underscores. Single
#     underscores are replaced with double underscores.
#     e.g. "U__my_2e_dotted_2e_name".
# If this value is left blank, Prometheus will default to `allow-utf-8` if the
# validation scheme for the current scrape config is set to utf8, or
# `underscores` if the validation scheme is set to `legacy`.
[ metric_name_escaping_scheme: <string> | default "allow-utf-8" ]

# Limit on total number of positive and negative buckets allowed in a single
# native histogram. The resolution of a histogram with more buckets will be
# reduced until the number of buckets is within the limit. If the limit cannot
# be reached, the scrape will fail.
# 0 means no limit.
[ native_histogram_bucket_limit: <int> | default = 0 ]

# Lower limit for the growth factor of one bucket to the next in each native
# histogram. The resolution of a histogram with a lower growth factor will be
# reduced as much as possible until it is within the limit.
# To set an upper limit for the schema (equivalent to "scale" in OTel's
# exponential histograms), use the following factor limits:
#
# +----------------------------+----------------------------+
# |        growth factor       | resulting schema AKA scale |
# +----------------------------+----------------------------+
# |          65536             |             -4             |
# +----------------------------+----------------------------+
# |            256             |             -3             |
# +----------------------------+----------------------------+
# |             16             |             -2             |
# +----------------------------+----------------------------+
# |              4             |             -1             |
# +----------------------------+----------------------------+
# |              2             |              0             |
# +----------------------------+----------------------------+
# |              1.4           |              1             |
# +----------------------------+----------------------------+
# |              1.1           |              2             |
# +----------------------------+----------------------------+
# |              1.09          |              3             |
# +----------------------------+----------------------------+
# |              1.04          |              4             |
# +----------------------------+----------------------------+
# |              1.02          |              5             |
# +----------------------------+----------------------------+
# |              1.01          |              6             |
# +----------------------------+----------------------------+
# |              1.005         |              7             |
# +----------------------------+----------------------------+
# |              1.002         |              8             |
# +----------------------------+----------------------------+
#
# 0 results in the smallest supported factor (which is currently ~1.0027 or
# schema 8, but might change in the future).
[ native_histogram_min_bucket_factor: <float> | default = 0 ]

# Specifies whether to convert classic histograms into native histograms with
# custom buckets (has no effect without --enable-feature=native-histograms).
[ convert_classic_histograms_to_nhcb: <bool> | default =
<global.convert_classic_histograms_to_nhcb>]
```



## ä¸‰ã€Exporterå’ŒPushGateway

### 1. Exporter
Exporter æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å°ç¨‹åºï¼Œè¿è¡Œåœ¨è¢«ç›‘æ§çš„ä¸»æœºæˆ–æœåŠ¡æ—è¾¹ã€‚å®ƒè´Ÿè´£é‡‡é›†æœ¬æœºæˆ–æœåŠ¡çš„è¿è¡ŒçŠ¶æ€æŒ‡æ ‡ï¼Œå¹¶é€šè¿‡ HTTP /metrics æ¥å£æš´éœ²å‡ºæ¥ï¼Œç„¶å Prometheus Server ä¼šå®šæœŸå»æ‹‰å–è¿™äº›æ•°æ®ã€‚æ¢å¥è¯è¯´ï¼ŒExporter æ˜¯ Prometheus çš„â€œæ•°æ®æ¢é’ˆâ€ï¼Œæ²¡æœ‰ Exporterï¼ŒPrometheus æœ¬èº«ä¸ä¼šä¸»åŠ¨äº§ç”ŸæŒ‡æ ‡æ•°æ®ã€‚ä½¿ç”¨ Exporter çš„é€šç”¨æ­¥éª¤ï¼ŒNode Exporter ä¸ºä¾‹ï¼ˆå®ƒç”¨æ¥ç›‘æ§ä¸»æœº CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œç­‰ï¼‰ï¼š

```yaml
â‘  ä¸‹è½½å¹¶è¿è¡Œ Exporter
# ä¸‹è½½ node_exporterï¼ˆä»¥Linuxä¸ºä¾‹ï¼‰
wget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz
tar xvf node_exporter-1.8.2.linux-amd64.tar.gz
cd node_exporter-1.8.2.linux-amd64

# å¯åŠ¨
./node_exporter
```

é»˜è®¤ä¼šåœ¨ :9100 ç«¯å£æä¾› /metrics æ¥å£ï¼Œåœ¨ Prometheus é…ç½®ä¸­æ·»åŠ  Scrape Jobï¼Œç¼–è¾‘ prometheus.ymlï¼š
```yaml
scrape_configs:
  - job_name: 'node_exporter' #job_nameï¼šä»»åŠ¡åç§°ï¼Œéšä¾¿èµ·ï¼Œä¾¿äºè¯†åˆ«
    static_configs:
      - targets: ['192.168.1.100:9100'] #targetsï¼šExporter æ‰€åœ¨ä¸»æœºå’Œç«¯å£
```

ğŸ§° å¸¸è§ç±»å‹çš„ Exporters
|Exporteråç§°|	ç”¨é€” |
|---|---|
|Node Exporter	| ç›‘æ§ä¸»æœº CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œç­‰ç³»ç»ŸæŒ‡æ ‡|
|Blackbox Exporter	| é€šè¿‡ ping/HTTP/TCP ç­‰æ–¹å¼æ¢æµ‹å¤–éƒ¨æœåŠ¡æ˜¯å¦å¯è¾¾|
|MySQL Exporter	| ç›‘æ§ MySQL æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡|
|Redis Exporter	| ç›‘æ§ Redis è¿è¡ŒçŠ¶æ€|
|Kafka Exporter	| ç›‘æ§ Apache Kafka æ¶ˆæ¯é˜Ÿåˆ—é›†ç¾¤æŒ‡æ ‡|
|Pushgateway	| æ¥æ”¶çŸ­å‘½ä»»åŠ¡æ¨é€çš„ä¸´æ—¶æŒ‡æ ‡ï¼ˆä¸æ˜¯è‡ªåŠ¨é‡‡é›†ï¼‰|

ğŸ“ å°ç»“
Exporter å°±åƒâ€œä¼ æ„Ÿå™¨â€ï¼ŒPrometheus å®šæœŸæ‹‰å–å®ƒä»¬çš„æ•°æ®ã€‚
ä½¿ç”¨æµç¨‹å›ºå®šï¼šéƒ¨ç½² â†’ æš´éœ² /metrics â†’ é…ç½® prometheus.yml â†’ é‡å¯ â†’ å¯è§†åŒ–ã€‚

### 2. Pushgateway
Prometheusåœ¨æ­£å¸¸æƒ…å†µä¸‹é€šè¿‡Pullæ¨¡å¼ä»äº§ç”Ÿmetricæˆ–è€…expoterå½“ä¸­æ‹‰å–ç›‘æ§æ•°æ®ã€‚ä½†å¦‚æœç›‘æ§Flinkæˆ–è€…Yarnä½œä¸šè®©Prometheuè‡ªåŠ¨å‘ç°ä½œä¸šçš„æäº¤ã€ç»“æŸå’Œè‡ªåŠ¨æ‹‰å–æ•°æ®å°±æ˜¾å¾—æ¯”è¾ƒå›°éš¾ã€‚Pushgatewayæ˜¯ä¸€ä¸ªä¸­è½¬ç»„ä»¶ï¼Œé€šè¿‡ç›¸å…³çš„é…ç½®å¯ä»¥å°†metricæ¨é€åˆ°PushGatewayï¼ŒPrometheuså†ä»ä¸­è‡ªåŠ¨æ‹‰å–å³å¯ã€‚

Pushgateway çš„å·¥ä½œåŸç†

- è¿™äº›ä»»åŠ¡å¯ä»¥**åœ¨å®Œæˆæ—¶å°†æŒ‡æ ‡æ•°æ®æ¨é€(push)åˆ° Pushgateway**ã€‚
- Pushgateway ä¼š**ä¸´æ—¶å­˜å‚¨**è¿™äº›æŒ‡æ ‡ï¼Œå¹¶æä¾›ä¸€ä¸ª **/metrics HTTP æ¥å£**ã€‚
- Prometheus **å®šæœŸä» Pushgateway æ‹‰å–**è¿™äº›æ•°æ®ã€‚

ğŸ“Œ æ‰€ä»¥ Pushgateway **å……å½“ä¸€ä¸ªä¸­è½¬ç¼“å­˜**ï¼ŒæŠŠâ€œä¸»åŠ¨æ¨é€â€è½¬æ¢æˆâ€œå¯è¢«æ‹‰å–â€ã€‚éœ€è¦æ³¨æ„çš„æ˜¯Pushgateway ä¸äº§ç”ŸæŒ‡æ ‡ï¼Œåªå­˜å‚¨ä½ æ¨é€æ¥çš„æ•°æ®ã€‚

âš™ï¸ æ¶æ„æµç¨‹å›¾
```yaml
[çŸ­å‘½ä»»åŠ¡è„šæœ¬] --push--> [Pushgateway] <--scrape-- [Prometheus] --> [Grafana]
```

åº”ç”¨å®ä¾‹

docker-compose.yml æ·»åŠ  Pushgatewayã€‚åœ¨docker-compose.yml ä¸­åŠ ä¸Šï¼š
```yaml
  pushgateway:
    image: prom/pushgateway:latest
    container_name: pushgateway
    ports:
      - "9091:9091"
```

åœ¨prometheus.yml æ·»åŠ é…ç½®
```yaml
scrape_configs:
  - job_name: 'pushgateway'
    static_configs:
      - targets: ['pushgateway:9091']
```

è¿™æ · Prometheus å°±ä¼šå®šæœŸæ‹‰å– Pushgateway ä¸­å­˜å‚¨çš„æŒ‡æ ‡ã€‚

âš¡ æ¨é€ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ curlï¼‰
æ¨¡æ‹Ÿä¸€ä¸ªçŸ­å‘½è„šæœ¬æ¨é€æ•°æ®ï¼š
```bash
echo "build_status 1" \  #build_status æ˜¯æŒ‡æ ‡åç§° 1 æ˜¯æŒ‡æ ‡å€¼
  | curl --data-binary @- http://localhost:9091/metrics/job/my_build 
  #job=my_build æ˜¯è¿™ä¸ªä»»åŠ¡çš„åå­—
```
æ¨é€å®Œæˆåï¼Œè®¿é—®ï¼šhttp://localhost:9091/
 å¯ä»¥çœ‹åˆ°ä½ æ¨é€çš„æ•°æ®ã€‚

